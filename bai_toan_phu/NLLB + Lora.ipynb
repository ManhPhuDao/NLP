{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14187311,"sourceType":"datasetVersion","datasetId":9045543}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"270b951f-0363-4add-91a8-1451d3151eff","cell_type":"code","source":"!pip install -q protobuf==3.20.3\n!pip install -q -U transformers datasets peft accelerate evaluate sacrebleu scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T08:59:32.892046Z","iopub.execute_input":"2025-12-23T08:59:32.892299Z","iopub.status.idle":"2025-12-23T09:01:25.394848Z","shell.execute_reply.started":"2025-12-23T08:59:32.892277Z","shell.execute_reply":"2025-12-23T09:01:25.393850Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m512.3/512.3 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m125.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.8.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.8.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"id":"7d798930","cell_type":"code","source":"import os\nimport gc\nimport torch\nimport numpy as np\nimport evaluate\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSeq2SeqLM, \n    DataCollatorForSeq2Seq, \n    Seq2SeqTrainer, \n    Seq2SeqTrainingArguments\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\n\n# =========================\n# 1. CONFIG & SETUP\n# =========================\n# XÃ³a bá»™ nhá»› cÅ©\ngc.collect()\ntorch.cuda.empty_cache()\n\nMODEL_ID = \"facebook/nllb-200-distilled-600M\"\nOUTPUT_DIR = \"./nllb_medical_finetune\"\n\n# ÄÆ°á»ng dáº«n file (Sá»­a láº¡i náº¿u cáº§n)\nTRAIN_VI_PATH = \"/kaggle/input/vlsp-data/MedicalDataset_VLSP/train.vi.txt\"\nTRAIN_EN_PATH = \"/kaggle/input/vlsp-data/MedicalDataset_VLSP/train.en.txt\"\n\n# Cáº¥u hÃ¬nh\nMAX_LENGTH = 128\nBATCH_SIZE = 16   \nMAX_SAMPLES = 250000 \n\n# Load metric BLEU\nmetric = evaluate.load(\"sacrebleu\")\n\n# =========================\n# 2. LOAD DATA & PREPROCESS\n# =========================\n# NLLB cáº§n src_lang (nguá»“n) vÃ  tgt_lang (Ä‘Ã­ch)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, src_lang=\"vie_Latn\", tgt_lang=\"eng_Latn\")\n\ndef load_data():\n    if os.path.exists(TRAIN_VI_PATH) and os.path.exists(TRAIN_EN_PATH):\n        print(\"ğŸ“‚ Äang Ä‘á»c file dá»¯ liá»‡u...\")\n        data = []\n        with open(TRAIN_VI_PATH, encoding=\"utf-8\") as f_vi, \\\n             open(TRAIN_EN_PATH, encoding=\"utf-8\") as f_en:\n            for i, (vi, en) in enumerate(zip(f_vi, f_en)):\n                if i >= MAX_SAMPLES: break\n                if vi.strip() and en.strip():\n                    data.append({\"vi\": vi.strip(), \"en\": en.strip()})\n        return Dataset.from_list(data)\n    else:\n        # Dá»¯ liá»‡u giáº£ láº­p náº¿u khÃ´ng tÃ¬m tháº¥y file\n        print(\"âš ï¸ KhÃ´ng tháº¥y file, táº¡o dá»¯ liá»‡u giáº£ láº­p...\")\n        dummy = [{\"vi\": \"Bá»‡nh nhÃ¢n Ä‘au Ä‘áº§u.\", \"en\": \"The patient has a headache.\"}] * 100\n        return Dataset.from_list(dummy)\n\nraw_dataset = load_data()\n\n# Chia táº­p Train (90%) vÃ  Test (10%) Ä‘á»ƒ cháº¥m Ä‘iá»ƒm BLEU\nsplit_dataset = raw_dataset.train_test_split(test_size=0.1, seed=42)\ntrain_ds = split_dataset[\"train\"]\ntest_ds = split_dataset[\"test\"]\n\nprint(f\"âœ… Train size: {len(train_ds)} | Test size: {len(test_ds)}\")\n\ndef preprocess_function(examples):\n    inputs = [ex for ex in examples[\"vi\"]]\n    targets = [ex for ex in examples[\"en\"]]\n    model_inputs = tokenizer(inputs, max_length=MAX_LENGTH, truncation=True)\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, max_length=MAX_LENGTH, truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Map dá»¯ liá»‡u\ntokenized_train = train_ds.map(preprocess_function, batched=True, remove_columns=[\"vi\", \"en\"])\ntokenized_test = test_ds.map(preprocess_function, batched=True, remove_columns=[\"vi\", \"en\"])\n\n# =========================\n# 3. MODEL & LORA\n# =========================\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID, device_map=\"auto\")\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"fc1\", \"fc2\"] # Target full\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# =========================\n# 4. HÃ€M TÃNH BLEU (QUAN TRá»ŒNG)\n# =========================\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    \n    # Decode káº¿t quáº£ dá»± Ä‘oÃ¡n\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    \n    # Thay -100 báº±ng pad_token_id Ä‘á»ƒ decode Ä‘Æ°á»£c nhÃ£n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Chuáº©n hÃ³a vÄƒn báº£n nháº¹\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [[label.strip()] for label in decoded_labels]\n    \n    # TÃ­nh BLEU\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    return {\"bleu\": result[\"score\"]}\n\n# =========================\n# 5. TRAINING ARGS\n# =========================\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=2,\n    learning_rate=2e-4,\n    num_train_epochs=1,\n    logging_steps=100,\n    fp16=True, # TÄƒng tá»‘c\n    save_strategy=\"no\",\n    eval_strategy=\"no\", # Táº¯t eval trong lÃºc train cho nhanh, sáº½ eval sau\n    predict_with_generate=True,\n    report_to=\"none\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# =========================\n# 6. TRAIN & EVALUATE\n# =========================\nprint(\"\\nğŸš€ Báº®T Äáº¦U TRAINING...\")\ntrainer.train()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T09:01:25.396733Z","iopub.execute_input":"2025-12-23T09:01:25.396999Z","iopub.status.idle":"2025-12-23T10:43:09.384901Z","shell.execute_reply.started":"2025-12-23T09:01:25.396976Z","shell.execute_reply":"2025-12-23T10:43:09.384283Z"}},"outputs":[{"name":"stderr","text":"2025-12-23 09:01:39.678620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766480500.056230      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766480500.168364      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1517a5bfd69f41eb9e53b0b021b44fc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab1ccaf7715f408587ddb1f5bbb7818d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca4cced757434b57be5d3f77ab531760"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efb1c65f9a2241109717d937f0b98f21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c0c4b6c7c2a442b9a63776cdf9dfbbe"}},"metadata":{}},{"name":"stdout","text":"ğŸ“‚ Äang Ä‘á»c file dá»¯ liá»‡u...\nâœ… Train size: 225000 | Test size: 25000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/225000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cae718ba50ae4937b608302b4671bb93"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c02e208f2ffd40d492478daa5dfd0c40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c7fe59eb98847a987463e75a24ed655"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a88ff6ca134f42e19d06ef6067ff65ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c706960ddf949db81ab3bc0ec809ba5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cde69c953d742aea0f17896e113f11f"}},"metadata":{}},{"name":"stdout","text":"trainable params: 8,650,752 || all params: 623,724,544 || trainable%: 1.3870\n\nğŸš€ Báº®T Äáº¦U TRAINING...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/1855590104.py:140: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7032' max='7032' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7032/7032 1:40:08, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.918200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.862800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.796600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.750300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.751100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.733800</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.722000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.693100</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.705100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.654900</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>1.659700</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.654500</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.680500</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.661800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.629100</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.640200</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>1.665800</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.623600</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>1.635300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.606100</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>1.601400</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>1.616400</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>1.627200</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.598200</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.620200</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>1.571500</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>1.583900</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>1.576500</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>1.594500</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.548100</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>1.599900</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>1.564800</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>1.577000</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>1.577200</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.583600</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>1.561200</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>1.557300</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>1.584400</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>1.572900</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.557300</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>1.584000</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>1.551200</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>1.551400</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>1.553000</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>1.547600</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>1.556700</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>1.543000</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>1.573200</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>1.541600</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.536300</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>1.549300</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>1.533200</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>1.551400</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>1.540100</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>1.530200</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>1.550600</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>1.518800</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>1.526200</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>1.528500</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>1.563700</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>1.515300</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>1.547600</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>1.535700</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>1.547200</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>1.562800</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>1.533400</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>1.539000</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>1.508500</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>1.515300</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>1.530500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=7032, training_loss=1.5993203015593267, metrics={'train_runtime': 6010.2299, 'train_samples_per_second': 37.436, 'train_steps_per_second': 1.17, 'total_flos': 4.41436787884032e+16, 'train_loss': 1.5993203015593267, 'epoch': 1.0})"},"metadata":{}}],"execution_count":2},{"id":"92afdf2f-1a10-4260-ae9a-20d7721837b0","cell_type":"code","source":"!pip install -q evaluate jiwer nltk\nimport nltk\nimport pandas as pd\nimport evaluate\n\n# Táº£i dá»¯ liá»‡u cáº§n thiáº¿t cho METEOR\nnltk.download(\"wordnet\")\nnltk.download(\"punkt\")\nnltk.download('omw-1.4') # Quan trá»ng cho wordnet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T10:52:33.828841Z","iopub.execute_input":"2025-12-23T10:52:33.829581Z","iopub.status.idle":"2025-12-23T10:52:40.816510Z","shell.execute_reply.started":"2025-12-23T10:52:33.829555Z","shell.execute_reply":"2025-12-23T10:52:40.815828Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":8},{"id":"6b507d61-01aa-4a73-8037-f8540394a879","cell_type":"code","source":"# Load cÃ¡c metrics\nbleu_metric = evaluate.load(\"sacrebleu\")\nmeteor_metric = evaluate.load(\"meteor\")\nter_metric = evaluate.load(\"ter\")\n\ndef generate_comprehensive_report(dataset, max_samples=100):\n    print(f\"ğŸš€ Äang cháº¡y Ä‘Ã¡nh giÃ¡ trÃªn {max_samples} máº«u...\")\n    \n    # Láº¥y máº«u\n    subset = dataset.select(range(min(len(dataset), max_samples)))\n    inputs = subset[\"input_ids\"]\n    labels = subset[\"labels\"]\n    \n    model.eval()\n    predictions = []\n    \n    batch_size = 8\n    total_batches = (len(inputs) + batch_size - 1) // batch_size\n    \n    # Thanh tiáº¿n trÃ¬nh Ä‘Æ¡n giáº£n\n    print(f\"ğŸ”„ Äang xá»­ lÃ½ {total_batches} batches...\")\n\n    for i in range(total_batches):\n        # 1. Láº¥y batch thÃ´ (cÃ¡c list Ä‘á»™ dÃ i khÃ¡c nhau)\n        batch_input_ids = inputs[i*batch_size : (i+1)*batch_size]\n        \n        # 2. FIX Lá»–I á» ÄÃ‚Y: DÃ¹ng tokenizer Ä‘á»ƒ Pad cho báº±ng nhau\n        # Chuyá»ƒn list of list thÃ nh list of dict Ä‘á»ƒ tokenizer hiá»ƒu\n        batch_data = [{\"input_ids\": ids, \"attention_mask\": [1]*len(ids)} for ids in batch_input_ids]\n        \n        # Pad tá»± Ä‘á»™ng thÃ nh tensor vuÃ´ng vá»©c\n        padded_inputs = tokenizer.pad(\n            batch_data,\n            padding=True,\n            return_tensors=\"pt\"\n        ).to(\"cuda\")\n        \n        input_ids = padded_inputs[\"input_ids\"]\n        attention_mask = padded_inputs[\"attention_mask\"]\n        \n        # 3. Dá»± Ä‘oÃ¡n\n        with torch.no_grad():\n            generated_tokens = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask, # ThÃªm mask cho chuáº©n\n                forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"eng_Latn\"),\n                max_length=128\n            )\n        \n        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n        predictions.extend(decoded_preds)\n\n    # Xá»­ lÃ½ nhÃ£n (Ground Truth)\n    clean_labels = []\n    for lbl in labels:\n        lbl_clean = [l if l != -100 else tokenizer.pad_token_id for l in lbl]\n        clean_labels.append(tokenizer.decode(lbl_clean, skip_special_tokens=True))\n    \n    # Decode Input Ä‘á»ƒ lÆ°u file\n    # (Pháº£i pad input toÃ n cá»¥c Ä‘á»ƒ decode hÃ ng loáº¡t, hoáº·c decode tá»«ng cÃ¡i)\n    # CÃ¡ch nhanh: decode tá»«ng cÃ¡i trong loop input gá»‘c\n    input_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in inputs]\n\n    print(\"ğŸ“Š Äang tÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘...\")\n    \n    bleu_score = bleu_metric.compute(predictions=predictions, references=clean_labels)\n    meteor_score = meteor_metric.compute(predictions=predictions, references=clean_labels)\n    ter_score = ter_metric.compute(predictions=predictions, references=clean_labels)\n    \n    print(\"\\n\" + \"=\"*40)\n    print(f\"ğŸ† Káº¾T QUáº¢ ÄÃNH GIÃ (TrÃªn {max_samples} cÃ¢u):\")\n    print(f\"ğŸ”¹ BLEU:   {bleu_score['score']:.2f}\")\n    print(f\"ğŸ”¹ METEOR: {meteor_score['meteor']:.4f}\")\n    print(f\"ğŸ”¹ TER:    {ter_score['score']:.2f}\")\n    print(\"=\"*40)\n\n    print(\"\\nğŸ“ Äang táº¡o file bÃ¡o cÃ¡o lá»—i (error_analysis.csv)...\")\n    df = pd.DataFrame({\n        \"Source (VI)\": input_texts,\n        \"Target (EN)\": clean_labels,\n        \"Model Output\": predictions\n    })\n    df.to_csv(\"error_analysis.csv\", index=False, encoding=\"utf-8-sig\")\n    print(\"âœ… Xong!\")\n    \n    return df\n\n# Cháº¡y láº¡i\nreport_df = generate_comprehensive_report(tokenized_test, max_samples=100)\nprint(report_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T10:53:36.914524Z","iopub.execute_input":"2025-12-23T10:53:36.915291Z","iopub.status.idle":"2025-12-23T10:54:20.843279Z","shell.execute_reply.started":"2025-12-23T10:53:36.915263Z","shell.execute_reply":"2025-12-23T10:54:20.842665Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\nYou're using a NllbTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"ğŸš€ Äang cháº¡y Ä‘Ã¡nh giÃ¡ trÃªn 100 máº«u...\nğŸ”„ Äang xá»­ lÃ½ 13 batches...\nğŸ“Š Äang tÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘...\n\n========================================\nğŸ† Káº¾T QUáº¢ ÄÃNH GIÃ (TrÃªn 100 cÃ¢u):\nğŸ”¹ BLEU:   33.60\nğŸ”¹ METEOR: 0.6199\nğŸ”¹ TER:    62.31\n========================================\n\nğŸ“ Äang táº¡o file bÃ¡o cÃ¡o lá»—i (error_analysis.csv)...\nâœ… Xong!\n                                         Source (VI)  \\\n0  PhÆ°Æ¡ng phÃ¡p: NghiÃªn cá»©u theo phÆ°Æ¡ng phÃ¡p mÃ´ táº£...   \n1  Káº¿t quáº£: KhÃ´ng cÃ³ tá»­ vong sau má»• vÃ  biáº¿n chá»©ng...   \n2  Káº¿t quáº£: MÃ´ hÃ¬nh lÃ­ thuyáº¿t trÃªn Ä‘iá»‡n toÃ¡n cáº¯t ...   \n3                 BMI nÃªn Ä‘Æ°á»£c tÃ­nh toÃ¡n vÃ  ghi láº¡i.   \n4  Má»™t thá»­ nghiá»‡m lÃ¢m sÃ ng khÃ´ng lÃ m mÃ¹ gáº§n Ä‘Ã¢y v...   \n\n                                         Target (EN)  \\\n0  Material and method: Cross sectional and descr...   \n1  Results: No operated mortality and 13.2% minim...   \n2  Results: In the theoretical arthroplasty model...   \n3  Body mass index (BMI) should be calculated and...   \n4  A recent unblinded clinical trial of dexametha...   \n\n                                        Model Output  \n0        Methods: Cross-sectional descriptive study.  \n1  Results: No postoperative mortality and mild c...  \n2  Results: The theoretical model on computed tom...  \n3             BMI should be calculated and recorded.  \n4  A recent nonblind clinical trial of dexamethas...  \n","output_type":"stream"}],"execution_count":10},{"id":"4dd9d7e3-3592-49df-a5c9-5af186a1aa0f","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}